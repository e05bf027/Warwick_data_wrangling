---
title: "UHG data wrangling"
author: "David M Hannon"
date: "08/11/2021"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# UHG Metavision processing

## Problem outline

There are two ways to get the version of Metavision in UHG to generate output.  

The first is a ‘Metavision Query Wizard’ that allows us to set parameters, and then returns the data that matches these parameters in the form of an Excel file. The limitation here is that if you ask for too many data points the machine hangs and you must abort the attempt.  

The second approach is a ‘back end’ method. This involves writing queries in SQL to interrogate the large relational databases in  Metavision. The issue here is that you must know the detailed structure of the database in order to write the SQL queries. Each deployment of Metavision is unique to the requirements of that  hospital (like each house is similar, and uses the same bricks to build with, but the results are subtly unique so that the instructions to locate the teabags in my house are different from someone else’s). The company who own Metavision, iMD Soft, were approached about in the past, but they wanted significant payment.

The final piece of the puzzle involves ABG data.  The ABG analysers do not integrate to Metavision. I am told this is a conscious choice on the part of the biochem department. The result is that the nurse at the patients bedside enters the results manually. The results in a myriad of potential issues:
* Is the time the nurse entered the same as the actual time the sample was taken? The key implication here is in matching the ABG value to position of the patient at this time.
* Have any errors been made when transcribing the values?
* Has a full result been recorded? Metavision does not have space for every result from the analyser (e.g. p50), and depending on clinical need, the nurse may only record the results that are most relevant at that moment.
* The Metavision data, and the biochem values are both available, but having both can lead to the appearance of two near identical samples at different times, when they refer to the same sample recorded in two ways.  

## Solution Outline

The SQL-based approach is not viable at present. The solution involves using a hybrid approach of multiple queries and data from the Wizard that are reshaped and joined or divided post-hoc. The steps involved are:

- **Load patient data**
  - Metavision data. This step depends on the approach to isolating data from Metavision initially:
    - A: assemble the fragments and create a data frame from each (they can be assembled later in various ways)
    - B: read the entirety of the data in and prepare filters that can 'punch out' bits you are interested in. These data frames can be reassembled as per point A above. Overall, this is the simplest approach.
  - ABG data:
    - This data can come from Metavision (higher likelihood of inaccuracy, especially regarding time the sample is taken) or from the Biochem department (more accurate, but slower to obtain, and reliant on the nurse running the sample through the analyser to have entered patient ID details with complete accuracy).
    
- **Ensure the data is anonymised**
  - Remove all possible patient identifiers.
  
- **Apply a transformation**
  - pivot any tables you have read in to a 'wide' format to render it tidy (NB ABG data from biochem already 'tidy').
  
- **Split the data**
  - If all the data has been loaded at once, filters must be applied to split it.

- **Combine the results**
  - assemble a new data table by adding/binding each variable table together
  - Apply a transformation to coerce the units of each variable correctly (I think this step is only partly automatable)
  
- **Output**
  - write the output to an Excel file/.csv/whatever is best, one sheet per area of interest e.g. CVS, ABGs, ventilator settings... n


## Solution details
### Scripts needed
1. **Read data in**
  - needs destination folder
  - reads files in destination folder
  - 1 frame per file
  - isolate only anonymous data
  
2. **Transform the data**
  - straightforward command to pivot wider
  
3. **Combine the results into new data frames**
  - assemble a new data table by adding/binding each variable table together
  - Apply a transformation to coerce the units of each variable correctly.

4. **Output** 
  - create an Excel file for each patient
  
5. **Final custom touches**
  - add an info sheet at the start of the file
  - add rows to indicate proning vs. unproning

### Scripts details

Step 1 is to load relevant packages:

```{r echo=TRUE, eval=FALSE}

library(tidyverse)   # Loads packages needed
library(lubridate)
library(readxl)

```

Step 2 is to create a character vector containing the destination for the large overall data file, and then we will create the large dataframe to be trimmed.

```{r }

# Begin by entering the pathway of the large data frame you will subdivide.
file_location <- as.character('/Users/davidhannon/Documents/02. Medicine/Med_Programming/MD_Metavision_wrangle/raw_data/01_AO/Verified/01_AO_verified.xlsx')

large_tibble <- read_xlsx(file_location, guess_max = 1000000)

```

The resulting tibble needs to be fully anonymised, we will select out only what is needed. Then we coerce 'parameter name' to character.

```{r}

large_tibble <-  large_tibble %>% 
                 select(Time, `Parameter Name`, Value)

large_tibble$`Parameter Name` <- as.factor(large_tibble$`Parameter Name`)
large_tibble$Value <- as.character(large_tibble$Value)

head(large_tibble)

```
Next, we pivot wider, and ensure we have sorted by Time. We also must isolate the portion relating to 

```{r}

all_variables <- unique(large_tibble$`Parameter Name`)   # isolates list of variables
head(all_variables)

```

The next step is optional and depends on whether we want to include the 30 minutes or so either side of the prone or supine positioning. This essentially follows a similar process to the above, but a little more involved.

```{r}

periprone_location <- as.character('/Users/davidhannon/Documents/02. Medicine/Med_Programming/MD_Metavision_wrangle/raw_data/01_AO/Unverified')

periprone_files <- list.files(periprone_location, full.names = TRUE)
periprone_files

```

This gives the files that we must cycle through and add to a new data frame.

```{r}


periprone <- read_xlsx(periprone_files[1], guess_max = 1000000)

for (i in 2:length(periprone_files)) {
  new_periprone <- read_xlsx(periprone_files[i], guess_max = 1000000) 
  periprone <- bind_rows(periprone, new_periprone)
}

periprone <- select(periprone, Time, `Parameter Name`, Value)
periprone$`Parameter Name` <- as.factor(periprone$`Parameter Name`)
periprone$Value <- as.character(periprone$Value)

```

Only after this frame is totally assembled does it make sense to pivot. If we are pursuing these fragments around proning then the next step it to assemble the complete data frame.

```{r}

large_tibble <- bind_rows(large_tibble, periprone)

```

First we remove the cardiac rhythm values as we must treat these separately before joining them back in to the pivoted main tibble.


```{r}

# Isolate the cardiac rhythm values and remove them from the larger data frame.
cardiac_rhythm <- filter(large_tibble, large_tibble$`Parameter Name` == 'Cardiac Rhythm')

large_tibble <- filter(large_tibble, large_tibble$`Parameter Name` != 'Cardiac Rhythm')

# Now manipulate the cardiac data independently. The initial pivot gives list_cols that are then turned to characters before the original column is removed.
cardiac_rhythm <- pivot_wider(cardiac_rhythm, names_from = `Parameter Name`, values_from = Value)

cardiac_rhythm$Cardiac_rhythm <- sapply(cardiac_rhythm$`Cardiac Rhythm`, toString)
cardiac_rhythm <- select(cardiac_rhythm, Time, Cardiac_rhythm)

### PIVOT THE LARGE TIBBLE
large_tibble <- pivot_wider(large_tibble, id_cols = Time, names_from = `Parameter Name`, values_from = Value) %>% arrange(Time)

# Rejoin the cardiac data
large_tibble <- left_join(large_tibble, cardiac_rhythm, by = 'Time')

```

## Generating the 'cookie cutter' filters

We must now create several filters that we use to punch out and isolate the data we want. Let;'s get a complete list of columns.

```{r}

colnames(large_tibble)

```

### Cardiovascular filter

```{r}

# Basic CV filter
cardio_basic <- select(large_tibble,
                       Time,
                       "Patient Positioning",
                       "Heart Rate",
                       "Cardiac_rhythm",
                       "Arterial Pressure Systolic",
                       "Arterial Pressure Diastolic",
                       "Arterial Pressure Mean",
                       "Central Venous Pressure",
                       "Non Invasive Arterial Pressure Systolic",
                       "Non Invasive Arterial Pressure Diastolic",
                       "Non Invasive Arterial Pressure Mean"
                       ) %>% arrange(Time)

```

```{r}

# Cardiac output (Vigileo)
cardio_output <- select(large_tibble,
                        Time,
                        "Patient Positioning",
                        "Cardiac output (Vigileo)",
                        "Stroke Volume(Vigileo)",
                        "Stroke Volume Index(Vigileo)",
                        "Stroke Volume Variation (Vigileo)",
                        "Systemic Vascular Resistance(Vigileo)",
                        "Systemic Vascular Resistance Index(Vigileo)",
                        "DO2(Vigileo)"
                        ) %>% arrange(Time)
                        
```

### Ventilation

```{r}

# Ventilator (PB) filter
ventilator <- select(large_tibble,
                     Time,
                     "Patient Positioning",
                     "Set Fraction Inspired Oxygen (PB)",
                     "PB Mode of ventilation",
                     "PB Ventilation Mode",
                     "PB Mandatory Mode Type",
                     "PB Spontaneous Type",
                     "PB Vent Type",
                     "Set PEEP (PB)",
                     "Total Respiratory Rate (PB)",
                     "Set Respiratory Rate (PB)",
                     "Set TV (PB)",
                     "Expiratory Tidal Volume (PB)",
                     "Minute Volume (PB)",
                     "Peak Inspiratory Pressure measured (PB)",
                     "Plateau Airway Pressure (PB)",
                     "Mean Airway Pressure (PB)",
                     "Peak Flow (Vmax) PB",
                     "Set I:E Ratio PB",
                     "Set Eof I:E Ratio",
                     "Set I of I:E Ratio",
                     "Set Pressure Control (PB)",
                     "Set Pressure Support (PB)",
                     "Inspiratory Time (PB)",
                     "Trigger Type Setting"
                     ) %>% arrange(Time)

```

### ABG filter

```{r}

# ABG (Metavision)
ABG_meta<- select(large_tibble,
                  Time,
                  "Patient Positioning",   # These two should be identical
                  "Patient Positioning ABG",
                  "Blood Gas Source",
                  "PH (ABG)",
                  "PaO2",
                  "PaCO2",
                  "Bicarbonate (ABG) s",   # Figure which of these is usually there
                  "Bicarbonate (ABG) a",
                  "Base excess (vt)",
                  "Base Excess (vv)",
                  "Lactate ABG",
                  "Total Haemoglobin",
                  "Sodium ABG",
                  "Chloride ABG",
                  "Potassium ABG",
                  "Ionised Calcium ABG",
                  "ABG Glucose Level",
                  ) %>% arrange(Time)
                  

```

